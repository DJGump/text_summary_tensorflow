{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"article_summarizer_graveyard.ipynb","provenance":[],"authorship_tag":"ABX9TyPp5XmUw1kgojLzyTahbdpa"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hjCwJriQ7dpj"},"source":["A note book to house code that I spent wya too much tim etrying to get to work"]},{"cell_type":"code","metadata":{"id":"RqcadCLo7XeO"},"source":["# from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# vocab_size = num_words_corpus\n","# text_df['encoded_docs'] = text_df['cleaned_text'].apply(lambda x: one_hot(x, vocab_size))\n","\n","# text_df['encoded_docs']\n","\n","# text_df['encoded_docs_length'] = text_df['encoded_docs'].apply(lambda x: len(x))\n","# max_length_article = text_df['encoded_docs_length'].max()\n","# max_length_article\n","\n","# # targets\n","# # vocab_size = num_words_sums\n","# text_df['encoded_sums'] = text_df['gensim_summaries'].apply(lambda x: one_hot(x, vocab_size))\n","\n","# text_df['encoded_sums_length'] = text_df['encoded_sums'].apply(lambda x: len(x))\n","# max_length_summary = text_df['encoded_sums_length'].max()\n","# max_length_summary\n","\n","# text_df['encoded_sums'].shape\n","\n","# padded_docs = pad_sequences(text_df['encoded_docs'], maxlen=max_length_summary, padding='post')\n","\n","# padded_docs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BKTFosLq-LaN"},"source":["### Text Embedding- One Hot Encoding\n","\n","# from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# vocab_size = num_words_corpus\n","# text_df['encoded_docs'] = text_df['cleaned_text'].apply(lambda x: one_hot(x, vocab_size))\n","\n","# text_df['encoded_docs']\n","\n","# text_df['encoded_docs_length'] = text_df['encoded_docs'].apply(lambda x: len(x))\n","# max_length_article = text_df['encoded_docs_length'].max()\n","# max_length_article\n","\n","# # targets\n","# # vocab_size = num_words_sums\n","# text_df['encoded_sums'] = text_df['gensim_summaries'].apply(lambda x: one_hot(x, vocab_size))\n","\n","# text_df['encoded_sums_length'] = text_df['encoded_sums'].apply(lambda x: len(x))\n","# max_length_summary = text_df['encoded_sums_length'].max()\n","# max_length_summary\n","\n","# text_df['encoded_sums'].shape\n","\n","# padded_docs = pad_sequences(text_df['encoded_docs'], maxlen=max_length_summary, padding='post')\n","\n","# padded_docs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bSX_p1-5-LXb"},"source":["# Model 1: Simple Sequential\n","# vocab_size = X_voc_size\n","# src_text_length = max_length_article\n","# sum_text_length = max_length_summary\n","\n","# # encoder input model\n","# inputs = Input(shape=(src_text_length,))\n","# encoder1 = Embedding(X_voc_size, 128, trainable=True)(inputs)\n","# encoder2 = LSTM(128)(encoder1)\n","# encoder3 = RepeatVector(sum_text_length)(encoder2)\n","\n","# # decoder ouput model\n","# decoder1 = LSTM(128, return_sequences=True)(encoder3)\n","# outputs = TimeDistributed(Dense(X_voc_size, activation='softmax'))(decoder1)\n","\n","# # add it all up\n","# model = Model(inputs=inputs, outputs=outputs)\n","# model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')\n","\n","# model.fit(X_train_seq_pad, \n","#           y = y_train_seq_pad.reshape(y_train_seq_pad.shape[0],y_train_seq_pad.shape[1], 1),\n","#           batch_size=20,\n","#           epochs=5,\n","#           #validation_data = (X_val_seq_pad, y_val_seq_pad.reshape(y_val_seq_pad.shape[0],y_val_seq_pad.shape[1], 1))\n","#           )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ThuWcue-LUf"},"source":["# Model 2: Recursive Model\n","# inputs1 = Input(shape=(src_text_length,))\n","# article1 = Embedding(vocab_size, 128)(inputs1)\n","# article2 = LSTM(128)(article1)\n","# article3 = RepeatVector(sum_text_length)(article2)\n","# # summary input model\n","# inputs2 = Input(shape=(sum_text_length,))\n","# summ1 = Embedding(vocab_size, 128)(inputs2)\n","# # decoder model\n","# decoder1 = concatenate([article3, summ1])\n","# decoder2 = LSTM(128, return_sequences=True)(decoder1)\n","# outputs = TimeDistributed(Dense(1, activation='softmax'))(decoder2)\n","# # tie it together [article, summary] [word]\n","# model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","# model.compile(loss='categorical_crossentropy', optimizer='adam')\n","\n","# model.fit([X_train_seq_pad, \n","#           y_train_seq_pad.reshape(y_train_seq_pad.shape[0],y_train_seq_pad.shape[1], 1)],\n","#           y=y_train_seq_pad.reshape(y_train_seq_pad.shape[0],y_train_seq_pad.shape[1], 1),\n","#           batch_size=20,\n","#           epochs=1,\n","#           #validation_data = (X_val_seq_pad, y_val_seq_pad.reshape(y_val_seq_pad.shape[0],y_val_seq_pad.shape[1], 1))\n","#           )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eRhoZBBG-LRS"},"source":["# K.clear_session()\n","\n","# latent_dim = 300\n","# embedding_dim=100\n","\n","# # Encoder\n","# encoder_inputs = Input(shape=(src_text_length,))\n","\n","# #embedding layer\n","# enc_emb =  Embedding(X_voc_size, embedding_dim,trainable=True)(encoder_inputs)\n","\n","# #encoder lstm 1\n","# encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n","# encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n","\n","# #encoder lstm 2\n","# encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n","# encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n","\n","# #encoder lstm 3\n","# encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n","# encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n","\n","# # Set up the decoder, using `encoder_states` as initial state.\n","# decoder_inputs = Input(shape=(None,))\n","\n","# #embedding layer\n","# dec_emb_layer = Embedding(y_voc_size, embedding_dim,trainable=True)\n","# dec_emb = dec_emb_layer(decoder_inputs)\n","\n","# decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n","# decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n","\n","# # Attention layer\n","# attn_layer = AttentionLayer(name='attention_layer')\n","# attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n","\n","# # Concat attention input and decoder LSTM output\n","# decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n","\n","# #dense layer\n","# decoder_dense =  TimeDistributed(Dense(y_voc_size, activation='softmax'))\n","# decoder_outputs = decoder_dense(decoder_concat_input)\n","\n","# # Define the model \n","# model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","# model.summary()\n","\n","# model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HaTxXVvd-LJf"},"source":["# def fit_model():\n","#   history=model.fit([x_tr, y_tr[:,:-1]], \n","#                     y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:], \n","#                     epochs=50, \n","#                     batch_size=128,\n","#                     validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]),\n","#                     callbacks=[es]\n","#                     )\n","#   return history\n","\n","# for i in np.arange(0, len(X_train_seq_pad), 50):\n","#   if i+49 <= len(X_train_seq_pad) - 1:\n","#     x_tr = X_train_seq_pad[i:i+49]\n","#     y_tr = y_train_seq_pad[i:i+49]\n","#     history = fit_model()\n","#     plt.plot(history.history['loss'], label='train')\n","#     plt.plot(history.history['val_loss'], label='val')\n","#   else:\n","#     x_tr = X_train_seq_pad[i:(len(X_train_seq_pad) - 1)]\n","#     y_tr = y_train_seq_pad[i:(len(X_train_seq_pad) - 1)]\n","#     history = fit_model()\n","#     plt.plot(history.history['loss'], label='train')\n","#     plt.plot(history.history['val_loss'], label='val')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QFUEgHGL7dDm"},"source":[""]}]}